
# Shift rows dynamically based on column value

Below is my input dataframe:
+---+----------+--------+
|ID |date      |shift_by|
+---+----------+--------+
|1  |2021-01-01|2       |
|1  |2021-02-05|2       |
|1  |2021-03-27|2       |
|2  |2022-02-28|1       |
|2  |2022-04-30|1       |
+---+----------+--------+

I need to groupBy "ID" and shift based on the "shift_by" column. In the end, the result should look like below:
+---+----------+----------+
|ID |date1     |date2     |
+---+----------+----------+
|1  |2021-01-01|2021-03-27|
|2  |2022-02-28|2022-04-30|
+---+----------+----------+

I have implemented the logic using UDF, but it makes my code slow. I would like to understand if this logic can be implemented without using UDF.
Below is a sample dataframe:
from datetime import datetime
from pyspark.sql.types import *

data2 = [(1, datetime.date(2021, 1, 1), datetime.date(2021, 3, 27)),
    (2, datetime.date(2022, 2, 28), datetime.date(2022, 4, 30))
]
schema = StructType([
    StructField("ID", IntegerType(), True),
    StructField("date1", DateType(), True),
    StructField("date2", DateType(), True),
])
df = spark.createDataFrame(data=data2, schema=schema)


        