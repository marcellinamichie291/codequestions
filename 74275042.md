
# I have compiled my custom tflite model into EdgeTpu model. The question is the Coral Edge TPU is not performing well and not blinking continuousl

To make this question more simple i have a Edge Tpu compiled model to detect a person wearing mask or not. The thing when i interprete the model everything running fine but the performance is not as expected. The frame pretty much laggy and only around 3 to 4 FPS which running from raspberry pi. Am i not utilizing the coral well? Basically the code is simple running facial recognition using opencv caffe model then passing the face location into the edgetpu tflite model and getting recognition. But im not sure why my coral is not performing as it should be. Any idea hoping to get response. Thanks
from typing_extensions import runtime

import PIL
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.models import load_model
import tensorflow as tf
from imutils.video import VideoStream
from PIL import Image


import tflite_runtime.interpreter as tflite


import numpy as np
import imutils
import time
import cv2
import os

def set_input2(interpreter, image, resample=Image.NEAREST):
    """Copies data to input tensor."""
    image = image.resize((224,224), resample)
    image = np.expand_dims(image, axis=0)
    input_tensor(interpreter)[:, :] = image    

def input_tensor(interpreter):
    """Returns input tensor view as numpy array of shape (height, width, 3)."""
    tensor_index = interpreter.get_input_details()[0]['index']
    return interpreter.tensor(tensor_index)()[0]

def output_tensor2(interpreter):
    """Returns dequantized output tensor if quantized before."""
    output_details = interpreter.get_output_details()[0]
    output_data = np.squeeze(interpreter.tensor(output_details['index'])())
        
    if 'quantization' not in output_details:
        return output_data
    scale, zero_point = output_details['quantization']
   
    if scale == 0:
        return output_data - zero_point
    return scale * (output_data - zero_point)

def detect_and_predict_mask(frame, faceNet, model,input_details,output_details):

    (h, w) = frame.shape[:2]
    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),
        (104.0, 177.0, 123.0))

    faceNet.setInput(blob)
    detections = faceNet.forward()

    faces = []
    locs = []
    preds = []

    # loop over the detections
    for i in range(0, detections.shape[2]):

        confidence = detections[0, 0, i, 2]


        if confidence > 0.5:
    
            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            (startX, startY, endX, endY) = box.astype("int")

            (startX, startY) = (max(0, startX), max(0, startY))
            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))

            face = frame[startY:endY, startX:endX]
            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
            face = cv2.resize(face, (224, 224))
            # face = img_to_array(face)
            # face = preprocess_input(face)

            # add the face and bounding boxes to their respective
            # lists
            faces = Image.fromarray(face)
            set_input2(model, faces)
            model.invoke()
            output_data = output_tensor2(model)
            locs.append((startX, startY, endX, endY))
            preds.append((len(locs) - 1 - i, output_data))
            
    return (locs, preds)

base_dir = os.path.dirname(__file__)
prototxtPath = os.path.join(base_dir + r'/face_detector/deploy.prototxt')
weightsPath = os.path.join(base_dir + r'/face_detector/res10_300x300_ssd_iter_140000.caffemodel')

faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)

# tf lite delegate for coral
EDGETPU_SHARED_LIB = 'edgetpu.dll'

model_file = os.path.join(base_dir + r'/new_mask_detector_quant_v2_edgetpu.tflite')
# load the face mask detector model from disk
print("[INFO] loading face mask detector model...")
model = tflite.Interpreter(
      model_path=model_file,
      experimental_delegates=[
          tflite.load_delegate(EDGETPU_SHARED_LIB)
      ])
input_details = model.get_input_details()
output_details = model.get_output_details()
model.allocate_tensors()

# initialize the video stream and allow the camera sensor to warm up
print("[INFO] starting video stream...")
vs = VideoStream(src=0).start()
time.sleep(2.0)

prev_frame_time = 0
new_frame_time = 0

# loop over the frames from the video stream
while True:

    # grab the frame from the threaded video stream and resize it
    # to have a maximum width of 400 pixels
    frame = vs.read()
    frame = imutils.resize(frame, width=500)


    # detect faces in the frame and determine if they are wearing a
    # face mask or not
    (locs, preds) = detect_and_predict_mask(frame, faceNet, model, input_details,output_details)

    # time to finish the frame and display the fps
    new_frame_time = time.time()
    fps = 1/(new_frame_time-prev_frame_time)
    prev_frame_time = new_frame_time
    fps = str(int(fps)) + " fps"
    cv2.putText(frame, fps, (7, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 0), 1, cv2.LINE_AA)

    # loop over the detected face locations and their corresponding
    # locations
    for (box, pred) in zip(locs, preds):
        # unpack the bounding box and predictions
        (startX, startY, endX, endY) = box
        output = list( pred)
        

        print(output)

    # Display FPS on frame  
    # show the output frame
    cv2.imshow("Mask Detector1", frame)
    key = cv2.waitKey(1) & 0xFF

    # if the `q` key was pressed, break from the loop
    if key == ord("q"):
        break

# do a bit of cleanup
cv2.destroyAllWindows()
vs.stop()





        