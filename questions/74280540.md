
# Total Gini impurity or entropy gain for a scikit-learn decision tree

How can I get the total weighted Gini impurity (or entropy) on a trained decision tree in scikit-learn? For instance, the following code on the titanic dataset,
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import tree
df_titanic = pd.read_csv('titanic_data.csv')    # a popular dataset
feat_list = ['SibSp','Pclass']  # number of siblings and spouses aboard; passenger class (1st,2nd,3rd)
clf = tree.DecisionTreeClassifier()
clf = clf.fit(df_titanic.loc[:,feat_list],df_titanic['Survived'])
fig = plt.figure(figsize=(10,10))
tree.plot_tree(clf,feature_names=feat_list,class_names=['NS','S'])
fig.show()

produces a tree with leaves with gini impurity values and sample sizes of (no particular order) (0.378,71), (0.32,5), (0.5,8),... I'm interested in the weighted sum, 0.378(71/891) + 0.32(5/891) + 0.5(8/891) + ... where 891 is the total number of samples (passengers). What's an easy way to do this?
I'd like to compare the total Gini impurity (or entropy) before and after the tree is constructed (as in Provost and Fawcett), but after researching the docs a bit there doesn't seem to be a tree attribute or method that directly produces this info.

        