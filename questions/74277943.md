
# Python lazy load large files on access

I have a large list of files (10-1000) that I want to lazily load the contents into memory on access. I have enough memory to load the contents of each individual file into memory, but not enough to load the contents of all the files into memory simultaneously. (To be specific, these are pickle files around 1-8GB each, but the solution should ideally be general to any type of file)
In the end, I want to be able to write code similar to this
a = file_contents[0]['property1'] # transparently loads file0 into memory
b = file_contents[0]['property2'] # keeps file0 in memory, no disk access
c = file_contents[5]['property1'] # unloads file0, then loads file5 into memory
d = file_contents[0]['property3'] # unloads file5, then loads file0 back into memory

where the interface transparently loads and unloads files as necessary to get the requested data structures, while keeping memory usage reasonable.
I can of course write my own class, but I'm interested if there is already a more robust implementation or better idiom for this kind of behavior

        