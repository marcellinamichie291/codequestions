
# Multiple case when then exists in apache spark scala

I've a sql that I want to convert to spark scala code,but don't know the sysntax of case when then case exists.
I was able to convert the first case when then sql . Could anyone guide me here :
Sql I want to convert 

SELECT PARTNER_TYPE_CD AS "Partner Type", COUNT(*) AS "Total"
  ,SUM(CASE
      WHEN  PI.SOFT_DEL_ID = 0 AND                                      
           (ARCHIVAL_DT >= CURRENT DATE OR                            
            YEAR(ARCHIVAL_DT) = '0001')                               
      THEN 1
      ELSE 0 END) AS "ActiveRecords"
   ,
   SUM(CASE
      WHEN  PI.SOFT_DEL_ID = 0 AND                                      
           (ARCHIVAL_DT < (CURRENT DATE - 5 YEAR) AND              
           YEAR(ARCHIVAL_DT) <> '0001')                                
      THEN
           (CASE                                                 
            WHEN EXISTS                                        
                 (SELECT 1                                     
                    FROM test_db.partner_info_association PIA  
                   WHERE PIA.ID = PI.ID
                     AND PIA.SOFT_DEL_IN = 'N' 
                   FETCH FIRST ROW ONLY                                       
                 )                                            
            THEN 1                                          
            ELSE 0                                          
            END)
      ELSE 0 END) AS "ArchivePinned"
      FROM test_db.partner_info PI                                                        
GROUP BY PARTNER_TYPE_CD    

Spark scala code I was able to write
val piDF=spark.read.table("test_db.partner_info")


    
    val piReportDF=piDF.groupBy("PARTNER_TYPE_CD")
                   .agg(count("*").alias("total")
                   ,sum(when((col("SOFT_DEL_ID") === "0") 
                   && (col("ARCHIVAL_DT") >= current_date() 
                   || year(col("ARCHIVAL_DT")) === "0001") 
                     , 1).otherwise(0))
                     .alias("ActiveRecords"))

Any code of spark sql and spark dsl will work for me.

        