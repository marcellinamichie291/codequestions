
# Kops breaks my Kubernetes cluster on forced rolling-update after editing cluster, nodeup logs returns 'nodeup config hash mismatch' [closed]







Closed. This question is not about programming or software development. It is not currently accepting answers.
                        
                    










 This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.


Closed 3 hours ago.







                        Improve this question
                    



**1. What kops version are you running? **
Version 1.22.0 (git-8ba0d78c0be251c7dd5584d178c3e342037705df)

2. What Kubernetes version are you running?
kubernetesVersion: v1.22.5

3. What cloud provider are you using?
AWS

Here is the Updated Cluster Manifest, after the edit
`apiVersion: kops.k8s.io/v1alpha2
kind: Cluster
metadata:
  creationTimestamp: "2022-10-31T04:48:18Z"
  generation: 2
  name: <CLUSTERNAME>
spec:
  api:
    loadBalancer:
      class: Classic
      type: Public
  authorization:
    rbac: {}
  channel: stable
  cloudProvider: aws
  configBase: s3://kops.rbactest.state.company/<CLUSTERNAME>
  dnsZone: cluster.local
  etcdClusters:
  - cpuRequest: 200m
    etcdMembers:
    - encryptedVolume: true
      instanceGroup: master-<REGION>-2c
      name: c
    memoryRequest: 100Mi
    name: main
  - cpuRequest: 100m
    etcdMembers:
    - encryptedVolume: true
      instanceGroup: master-<REGION>-2c
      name: c
    memoryRequest: 100Mi
    name: events
  iam:
    allowContainerRegistry: true
    legacy: false
  kubeAPIServer:
    oidcClientID: xxxx
    oidcGroupsClaim: groups
    oidcGroupsPrefix: 'oidc:'
    oidcIssuerURL: xxxx
    oidcUsernameClaim: upn
  kubelet:
    anonymousAuth: false
  kubernetesApiAccess:
  - 0.0.0.0/0
  - ::/0
  kubernetesVersion: v1.22.5
  masterInternalName: api.internal.<CLUSTERNAME>
  masterPublicName: api.<CLUSTERNAME>
  networkCIDR: 172.20.0.0/16
  networking:
    weave:
      mtu: 8912
  nonMasqueradeCIDR: xxx/10
  sshAccess:
  - 0.0.0.0/0
  - ::/0
  subnets:
  - cidr: xxx/19
    name: <REGION>-2c
    type: Private
    zone: <REGION>-2c
  - cidr: xxx/22
    name: utility-<REGION>-2c
    type: Utility
    zone: <REGION>-2c
  topology:
    dns:
      type: Public
    masters: private
    nodes: private

`

What commands did you run?  What is the simplest way to reproduce this issue?
`kops create cluster  --name <CLUSTERNAME> --master-zones <REGION>-2c --dns-zone cluster.local --zones <REGION>-2c --topology private --master-count 1 --master-size t3.medium --node-count 1 --node-size t3.medium --node-volume-size 9 --ssh-public-key ~/.ssh/id_rsa.pub --networking weave --kubernetes-version v1.22.5 --state s3://kops.rbactest.state.company --target terraform --out=. --yes

terraform plan  -out createcluster.out
terraform apply "createcluster.out";
kops edit cluster  --name <CLUSTERNAME> --state s3://kops.rbactest.state.company`

Added the following block in the 'spec', the change was successfully updated
   kubeAPIServer:
    oidcClientID: xxx
    oidcGroupsClaim: groups
    oidcGroupsPrefix: 'oidc:'
    oidcIssuerURL: xxx
    oidcUsernameClaim: upn

kops update cluster --name <CLUSTERNAME> --state s3://kops.rbactest.state.company --yes
Checked the kube-apiserver.manifest on the master node, it had not added the above parameters
kops rolling-update cluster --name <CLUSTERNAME> --state s3://kops.rbactest.state.company --yes


NAME                    STATUS  NEEDUPDATE      READY   MIN     TARGET  MAX     NODES
master-<REGION>-2c       Ready   0               1       1       1       1       1
nodes-<REGION>-2c        Ready   0               1       1       1       1       1

No rolling-update required.

Tried a forced rolling-update on the cluster so that the restarted nodes take up the added parameters
kops rolling-update cluster --name <CLUSTERNAME> --state s3://kops.rbactest.state.company --yes --force
I1031 10:41:33.647942     596 instancegroups.go:470] Validating the cluster.
I1031 10:41:36.700701     596 instancegroups.go:503] Cluster validated.
I1031 10:41:36.700820     596 instancegroups.go:306] Tainting 1 node in "master-<REGION>-2c" instancegroup.
I1031 10:41:37.000232     596 instancegroups.go:398] Draining the node: "xxxx".
evicting pod kube-system/ebs-csi-controller
evicting pod kube-system/dns-controller
I1031 10:41:42.194147     596 instancegroups.go:658] Waiting for 5s for pods to stabilize after draining.
I1031 10:41:47.198085     596 instancegroups.go:417] deleting node "xxxx" from kubernetes
I1031 10:41:47.485789     596 instancegroups.go:591] Stopping instance "i-instancecode", node "xxxx", in group "xxx" (this may take a while).
I1031 10:41:48.030802     596 instancegroups.go:435] waiting for 15s after terminating instance
I1031 10:42:03.034976     596 instancegroups.go:470] Validating the cluster.
I1031 10:42:26.022372     596 instancegroups.go:516] Cluster did not validate, will retry in "30s": error listing nodes: Get "https://<K8S-API-ELB-URL>/api/v1/nodes": EOF.
I1031 10:43:12.869998     596 instancegroups.go:516] Cluster did not validate, will retry in "30s": error listing nodes: Get "https://<K8S-API-ELB-URL>/api/v1/nodes": EOF.
...
I1031 10:57:20.592087     596 instancegroups.go:513] Cluster did not validate within deadline: error listing nodes: Get "https://<K8S-API-ELB-URL>/api/v1/nodes": EOF.
E1031 10:57:20.592374     596 instancegroups.go:475] Cluster did not validate within 15m0s
Error: master not healthy after update, stopping rolling-update: "error validating cluster after terminating instance: cluster did not validate within a duration of \"15m0s\""

ANOTHER Scenario
This issue is not specific to this kind of cluster edit, The same thing happens when I attempt to rotate secrets through kops using this guide, Those changes too, eventually need a forced rolling update, but the cluster fails to come back up with the same error
Following are the commands i ran for distrusting the kubernetes-ca secret
kops create cluster  --name <clustername> --master-zones <REGION>-2c --dns-zone cluster.local --zones <REGION>-2c --topology private --master-count 1 --master-size t3.medium --node-count 1 --node-size t3.medium --node-volume-size 9 --ssh-public-key ~/.ssh/id_rsa.pub --networking weave --kubernetes-version v1.22.5 --state s3://kops_s3bucketname --target terraform --out=. --yes

 terraform plan -lock=false -out distrust.out;

 terraform apply -lock=false "distrust.out";

kops export kubecfg --name <clustername> --state s3://kops_s3bucketname --admin

cp ~/.kube/config ~/.kube/tobedistrustedconfig

kops create keypair kubernetes-ca --name <clustername> --state s3://kops_s3bucketname | awk '{print $3}'

kops update cluster --name <clustername> --state s3://kops_s3bucketname --yes

kops get keypairs  --name <clustername> --state s3://kops_s3bucketname --distrusted

OUTPUT:
NAME                    ID                              ISSUED          EXPIRES         DISTRUSTED      PRIMARY HASPRIVATE
apiserver-aggregator-ca somenumber2855973299883233449    2022-10-29      2032-10-28                      *       *
etcd-clients-ca         somenumber2996941425981774806    2022-10-29      2032-10-28                      *       *
etcd-manager-ca-events  somenumber1990900741898490408    2022-10-29      2032-10-28                      *       *
etcd-manager-ca-main    somenumber2943238014125571433    2022-10-29      2032-10-28                      *       *
etcd-peers-ca-events    somenumber2810580649751556520    2022-10-29      2032-10-28                      *       *
etcd-peers-ca-main      somenumber1934684343195917483    2022-10-29      2032-10-28                      *       *
kubernetes-ca           somenumber4830441058670623376    2022-10-29      2032-10-28                      *       *
kubernetes-ca           somenumber909115619086621902    2022-10-29      2032-10-28                              *
service-account         somenumber4018232252231655112    2022-10-29      2032-10-28                      *       *


kops promote keypair kubernetes-ca $newkeypairid --name <clustername> --state s3://kops_s3bucketname 
OUTPUT: Promoted kubernetes-ca $newkeypairid

kops update cluster --name <clustername> --state s3://kops_s3bucketname --yes

kops export kubecfg --name <clustername> --state s3://kops_s3bucketname --admin

kops distrust keypair kubernetes-ca $oldkeypairid --name <clustername> --state s3://kops_s3bucketname
OUTPUT: Distrusted kubernetes-ca $newkeypairid

kops update cluster --name <clustername> --state s3://kops_s3bucketname --yes

kops rolling-update cluster --name <clustername> --state s3://kops_s3bucketname --yes --force --cloudonly 

OUTPUT:
NAME                    STATUS          NEEDUPDATE      READY   MIN     TARGET  MAX
master-<REGION>-2c       NeedsUpdate     1               0       1       1       1
nodes-<REGION>-2c        NeedsUpdate     1               0       1       1       1
W1031 14:15:16.487288    8337 instancegroups.go:467] Not validating cluster as cloudonly flag is set.
W1031 14:15:16.487896    8337 instancegroups.go:393] Not draining cluster nodes as 'cloudonly' flag is set.
I1031 14:15:16.488753    8337 instancegroups.go:593] Stopping instance "i-0f2c02f126557de5f", in group "master-<REGION>-2c.masters.<CLUSTERNAME>" (this may take a while).
I1031 14:15:16.960638    8337 instancegroups.go:435] waiting for 15s after terminating instance
W1031 14:15:31.963754    8337 instancegroups.go:467] Not validating cluster as cloudonly flag is set.
W1031 14:15:31.963935    8337 instancegroups.go:467] Not validating cluster as cloudonly flag is set.
W1031 14:15:31.964135    8337 instancegroups.go:393] Not draining cluster nodes as 'cloudonly' flag is set.
I1031 14:15:31.964191    8337 instancegroups.go:593] Stopping instance "<INSTANCEID>", in group "nodes-<REGION>-2c.<CLUSTERNAME>" (this may take a while).
I1031 14:15:32.438623    8337 instancegroups.go:435] waiting for 15s after terminating instance
W1031 14:15:47.442180    8337 instancegroups.go:467] Not validating cluster as cloudonly flag is set.
I1031 14:15:47.442259    8337 rollingupdate.go:208] Rolling update completed for cluster "<CLUSTERNAME>"!

What happened after the commands executed?
The EC2 Instances for the new master and worker is created, but the kubernetes-api-elb shows the master as OutOfService, because NodeUp failed to run, and has not opened port 443 to listen for incoming HTTPS connections. Hence, this breaks the cluster
What did you expect to happen?
Clean rolling-upgrade of the cluster
The master node is being created, but the nodeup process is failing, went to the master node and got following logs
 journalctl -u kops-configuration.service

-- Logs begin at Mon 2022-10-31 05:13:44 UTC, end at Mon 2022-10-31 05:33:42 UTC. --
Oct 31 05:14:05 ip-xxx systemd[1]: Starting Run kOps bootstrap (nodeup)...
Oct 31 05:14:05 ip-xxx nodeup[1336]: nodeup version 1.22.0
Oct 31 05:14:05 ip-xxx nodeup[1336]: I1031 05:14:05.618512    1336 s3context.go:216] found bucket in region "us-east-1"
Oct 31 05:14:05 ip-xxx nodeup[1336]: I1031 05:14:05.618676    1336 s3fs.go:327] Reading file "s3://kops.rbactest.state.company/<CLUSTERNAME>/cluster-completed.spec"
Oct 31 05:14:05 ip-xxx nodeup[1336]: I1031 05:14:05.909522    1336 s3fs.go:327] Reading file "s3://kops.rbactest.state.company/<CLUSTERNAME>/igconfig/master/master-<REGION>-2c/nodeupconfig.yaml"
Oct 31 05:14:05 ip-xxx nodeup[1336]: W1031 05:14:05.989101    1336 main.go:133] got error running nodeup (will retry in 30s): nodeup config hash mismatch
Oct 31 05:14:36 ip-xxx nodeup[1336]: I1031 05:14:36.013316    1336 s3fs.go:327] Reading file "s3://kops.rbactest.state.company/<CLUSTERNAME>/cluster-completed.spec"
Oct 31 05:14:36 ip-xxx nodeup[1336]: I1031 05:14:36.298072    1336 s3fs.go:327] Reading file "s3://kops.rbactest.state.company/<CLUSTERNAME>/igconfig/master/master-<REGION>-2c/nodeupconfig.yaml"
Oct 31 05:14:36 ip-xxx nodeup[1336]: W1031 05:14:36.377894    1336 main.go:133] got error running nodeup (will retry in 30s): nodeup config hash mismatch

On updating kops to 1.25.0, got the same error but with the expected and current hashes.
Oct 29 21:25:36 ip-172-20-53-16 systemd[1]: Starting Run kOps bootstrap (nodeup)...
Oct 29 21:25:36 ip-172-20-53-16 nodeup[1326]: nodeup version 1.25.2 (git-v1.25.2)
Oct 29 21:25:36 ip-172-20-53-16 nodeup[1326]: I1029 21:25:36.480506    1326 s3context.go:215] found bucket in region "us-east-1"
Oct 29 21:25:36 ip-172-20-53-16 nodeup[1326]: I1029 21:25:36.480686    1326 s3fs.go:329] Reading file "s3://kops.rbactest.state.company/<CLUSTERNAME>/cluster-completed.spec"
Oct 29 21:25:36 ip-172-20-53-16 nodeup[1326]: I1029 21:25:36.787386    1326 s3fs.go:329] Reading file "s3://kops.rbactest.state.company/<CLUSTERNAME>/igconfig/master/master-<REGION>-2c/nodeupconfig.yaml"
Oct 29 21:25:36 ip-172-20-53-16 nodeup[1326]: W1029 21:25:36.902694    1326 main.go:133] got error running nodeup (will retry in 30s): nodeup config hash mismatch (was "somehash=", expected "someotherhash=")


        