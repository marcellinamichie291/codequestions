
# Pyspark groupby column and divide by max value

I have a dataframe which looks like this
columns = ['id', 'department', 'score']
vals = [
    (1, 'AB', 141),
    (2, 'AB', 140),
    (3, 'AB', 210),
    (4, 'AB', 120),
    (5, 'EF', 20),
    (6, 'EF', 15)
]

I'd like to find the max score of each group of department and divide all values of that group. For example, in the above case:
max_val for AB is 210
max_val for EF is 20
The new dataset should be:
    (1, 'AB', 0.67),
    (2, 'AB', 0.67),
    (3, 'AB', 1.00),
    (4, 'AB', 0.57),
    (5, 'EF', 1.00),
    (6, 'EF', 0.75)

For now I've tried
>>> max_distance = df.groupby("department").agg({"score": "max"}).collect()
>>> max_distance
[Row(department='AB', max(score)=210.0), Row(department='EF', max(score)=20.0)]

But how do I divide it across the entire group?

        