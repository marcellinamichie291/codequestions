
# Stream data into BigQuery data mart with three different dataflow jobs, get correct surrogate key for the fact table from dimensional tables

I have unbounded data being streamed into BigQuery with three separate Dataflow jobs, one fact table + 2 dimensional tables. I want to know how to proceed if I want to link the fact rows to the dimensional rows through a surrogate key for keeping track of the Slowly Changing Dimensions(SCD). I want to somehow store the information about the dimensional tables + surrogate keys in memory in Dataflow(Apache Beam) in order to do some transformation before inserting into fact table.

        