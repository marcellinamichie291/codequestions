
# Spark Sql MapType doesn't work correctly after worked for a period sometimes

env:

OS: CentOS Linux release 7.4.1708 (Core)
java: "1.8.0_144" Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)
scala: 2.12.8
spark: 3.1.1

I have a dataframe operation like this:
    df.select(
      org.apache.spark.sql.functions.map(lit("total"), lit("true")).as("extraInfo")
    ).union(
      df.select(
        org.apache.spark.sql.functions.map(lit("total"), lit("false")).as("extraInfo")
      )
    )

At first, it worked as expected. But after running for about a week, an exception thrown, the stack trace is below
java.lang.IllegalArgumentException: requirement failed
    at scala.Predef$.require(Predef.scala:268)
    at org.apache.spark.sql.catalyst.util.ArrayBasedMapData.<init>(ArrayBasedMapData.scala:29)
    at org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.build(ArrayBasedMapBuilder.scala:115)
    at org.apache.spark.sql.catalyst.expressions.CreateMap.eval(complexTypeCreator.scala:232)
    at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:66)
    at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:54)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:317)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:317)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:322)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:322)
    at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDown$1(QueryPlan.scala:94)
    at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
    at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)
    at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)
    at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)

I use a diagnostic tool to debug, and found there is a abnormal org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder instance,
its field keys and values is not empty, and length of keys and values are not the same, sometimes like this
keys: @ArrayBuffer[ArrayBuffer(total,null)]
values:@ArrayBuffer[ArrayBuffer(true)]
keyToIndex: empty

It's quite strange, I found that null key is not allowed to append to keys, but actually there is a null key. And size of keys/values/keyToIndex should be the same, but it wasn't.
Source code of org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder#put:
  def put(key: Any, value: Any): Unit = {
        if (key == null) {
      throw new RuntimeException("Cannot use null as map key.")
    }

    val index = keyToIndex.getOrDefault(key, -1)
    if (index == -1) {
      keyToIndex.put(key, values.length)
      keys.append(key)
      values.append(value)
    } 
  }

besides, I view the code, found that after org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder#build is called,
org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder#reset will be called, keys/values/keyToIndex should be empty, but it wasn't.
  private def reset(): Unit = {
    keyToIndex.clear()
    keys.clear()
    values.clear()
  }

I guess maybe some deserialization to generate the wrong org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder instance, but I didn't find any proof.
Can anyone give me some suggestion, thanks in advance!

        