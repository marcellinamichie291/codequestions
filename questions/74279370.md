
# Image classification through imagedatagenerator takes too long

I am currently classifying images using the cifar10 data set. I'm using colab (with gpu). By the way, my code takes several hours to go around one epoch. I have to use the ImageDataGenerator, but I think it's slow because of this. Is there anything wrong with my code? Or is there a way to speed it up?
import numpy as np 
import pandas as pd
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization
        
train_dir = '/content/drive/MyDrive/cifar10/train'
test_dir = '/content/drive/MyDrive/cifar10/test'

train_datagen = ImageDataGenerator(rescale = 1./255)
train_generator = train_datagen.flow_from_directory(train_dir,
                                                    batch_size = 64,
                                                    target_size = (32,32),
                                                    class_mode='categorical')

test_datagen = ImageDataGenerator(rescale = 1./255)
test_generator = test_datagen.flow_from_directory(test_dir,
                                                  target_size = (32, 32),
                                                  batch_size = 64,
                                                  class_mode = 'categorical')

model = Sequential()
model.add(Conv2D(filters=32, 
                 kernel_size=(3,3),
                 activation='relu',
                 padding='SAME',
                 input_shape=(32,32,3)))
model.add(Conv2D(filters=32, 
                 kernel_size=(3,3),
                 activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
# model.add(BatchNormalization())
model.add(Dropout(rate=0.25))
model.add(Flatten())   
model.add(Dense(units=64, activation = 'relu'))
model.add(Dropout(rate=0.5))
model.add(Dense(units=10, activation = 'softmax'))

model.summary()

model.compile(optimizer = 'adamax', loss = 'categorical_crossentropy', metrics = 'acc')

history = model.fit(train_generator,
                    epochs = 15,
                    batch_size = 64,
                    validation_split=0.1,
                    validation_data=test_generator,
                    shuffle=True)

model.evaluate(test_generator)


        