
# Kafka Consumer to ingest data without querying DB for every message

We have a Kafka consumer service to ingest data into our DB. Whenever we receive the message from the topic we will compose an insert statement to insert this message into DB. We use DB connection pool to handle the insertion and so far so good.
Currently, we need to add a filter to select only the related message from Kafka and do the insert. There are two options in my mind to do this.
Option 1:  Create a config table in the DB to define our filtering condition.
Pros

No need to make code changes or redeploy services
Just insert new filters to config table, service will pick them the next run

Cons

eed to query the DB every time we receive new messages.
Say we received 100k new messages daily and need to filter out 50k. So totally we only need to run 50k INSERT commands, while need to run 100K SELECT queries to check the filter condition for every single Kafka message.

Option 2: Use a hardcoded config file to define those filters.
Pros

Only need to read the filters once when the consumer start running
Has no burden on the DB layer

Cons

This is not a scalable way since we are planning to add a lot of filters, everytime we need to make code changes on the config file and redeploy the consumer.

My question is, is there a better option to achieve the goal? Find the filters without using hardcoded config file or without increasing the concurrency of DB queries.

        