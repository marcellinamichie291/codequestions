
# Read specific record(s) from Dynamo using Apache Beam DynamoDBIO

I have an Apache Beam pipeline that reads data from DynamoDB. To read the data I use the Apache Beam DynamoDBIO SDK. I need to read specific/filter data in my use case, meaning I have to use filterExpression in DynamoDBIO. My current code is as follows,
Map<String, AttributeValue> expressionAttributeValues = new HashMap<>();
expressionAttributeValues.put(":message", AttributeValue.builder().s("Ping").build());

pipeline
.apply(DynamoDBIO.<List<Map<String, AttributeValue>>>read()
                .withClientConfiguration(DynamoDBConfig.CLIENT_CONFIGURATION)
                .withScanRequestFn(input -> ScanRequest.builder().tableName("SiteProductCache").totalSegments(1)
                        .filterExpression("KafkaEventMessage = :message")
                        .expressionAttributeValues(expressionAttributeValues)
                        .projectionExpression("key, KafkaEventMessage")
                        .build())
                .withScanResponseMapperFn(new ResponseMapper())
                .withCoder(ListCoder.of(MapCoder.of(StringUtf8Coder.of(), AttributeValueCoder.of())))
                )
.apply(...)

----

static final class ResponseMapper implements SerializableFunction<ScanResponse, List<Map<String, AttributeValue>>> {
        @Override
        public List<Map<String, AttributeValue>> apply(ScanResponse input) {
            if (input == null) {
                return Collections.emptyList();
            }
            return input.items();
        }
}

When executing the code, am getting the below error,
Exception in thread "main" java.lang.IllegalArgumentException: Forbidden IOException when writing to OutputStream
    at org.apache.beam.sdk.util.CoderUtils.encodeToSafeStream(CoderUtils.java:89)
    at org.apache.beam.sdk.util.CoderUtils.encodeToByteArray(CoderUtils.java:70)
    at org.apache.beam.sdk.util.CoderUtils.encodeToByteArray(CoderUtils.java:55)
    at org.apache.beam.sdk.transforms.Create$Values$CreateSource.fromIterable(Create.java:413)
    at org.apache.beam.sdk.transforms.Create$Values.expand(Create.java:370)
    at org.apache.beam.sdk.transforms.Create$Values.expand(Create.java:277)
    at org.apache.beam.sdk.Pipeline.applyInternal(Pipeline.java:548)
    at org.apache.beam.sdk.Pipeline.applyTransform(Pipeline.java:499)
    at org.apache.beam.sdk.values.PBegin.apply(PBegin.java:56)
    at org.apache.beam.sdk.io.aws2.dynamodb.DynamoDBIO$Read.expand(DynamoDBIO.java:301)
    at org.apache.beam.sdk.io.aws2.dynamodb.DynamoDBIO$Read.expand(DynamoDBIO.java:172)
    at org.apache.beam.sdk.Pipeline.applyInternal(Pipeline.java:548)
    at org.apache.beam.sdk.Pipeline.applyTransform(Pipeline.java:482)
    at org.apache.beam.sdk.values.PBegin.apply(PBegin.java:44)
    at org.apache.beam.sdk.Pipeline.apply(Pipeline.java:177)
    at some_package.beam_state_storage.dynamodb.DynamoDBPipelineDefinition.run(DynamoDBPipelineDefinition.java:40)
    at some_package.beam_state_storage.dynamodb.DynamoDBPipelineDefinition.main(DynamoDBPipelineDefinition.java:28)
Caused by: java.io.NotSerializableException: software.amazon.awssdk.core.util.DefaultSdkAutoConstructList
    at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1197)
    at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1582)
    at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1539)
    at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1448)
Caused by: java.io.NotSerializableException: software.amazon.awssdk.core.util.DefaultSdkAutoConstructList

Does anyone have an idea how to solve this or the correct way to read and filter data, am a bit new to this Apache Beam stuff and appreciate any guidance.

        