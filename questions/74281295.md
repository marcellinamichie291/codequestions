
# Coding stochastic gradient descent from scratch in python - convergency problem

I am trying to code the stochastic gradient descent from scratch. My data test are observations with x_i a random list of 2D points and y_i a list of label -1 or 1 if the point are on one side or another of an hyperplan of normal vector omega (that is the blue line on the plot). This is the code to generate the data :
# Librairies used
import numpy as np 
import matplotlib.pyplot as plt
from matplotlib import cm
import pandas as pd
# from sklearn.metrics import mean_squared_error
# from sklearn.svm import SVC
import random as rd
%matplotlib inline
import csv 

# Generation of N point x_i with 2 coordonates between 0 and 100
N = 50
x_i = 100*np.random.random((N,2))

# Generation of the normal vector omega 
a = np.random.random() # a = rd.randrange(-1,1,0.01)
print(a)
b = 100*np.random.random() 
omega = [-a,1] # normal vector for the hyperplan y-ax-b=0 i.e y = ax+b
print("omega")
print(omega)

# Generation of the label y_i
y_i = [0]*len(x_i)
y_i = np.array(y_i)

for k in range(N):
    if np.dot(x_i[k],np.transpose(omega)) <= b :
        y_i[k] = -1
    elif np.dot(x_i[k],np.transpose(omega)) >= -b :
        y_i[k] = 1

# Plotting the points under the line and above the line
def plotting_points(x_i,y_i):
    x, y = x_i.T
    # Plotting of the line
    plt.grid()
    plt.plot(np.linspace(0,100),a*np.linspace(0,100)+b,color='mediumturquoise')
    
    # Plotting the points
    for k in range(N):
        if y_i[k] == -1:
            plt.scatter(x_i[k][0],x_i[k][1],color='palegreen')
        if y_i[k] == 1:
            plt.scatter(x_i[k][0],x_i[k][1],color='mediumpurple')
    plt.show()

plotting_points(x_i,y_i)

Then I try to implement stochastic gradient descent on this data to estimate the omega vector. The code is below :
# Implementation of stochastic gradient for the empirical risk 
def grad_sto_risk(x,y,omega,n):
    S = 0
    omega = omega/np.linalg.norm(omega,ord=2) # normalization of omega
    while np.linalg.norm(omega,ord=2) < 2000: # stop criterion because there is a problem of convergency
        for t in range (1,100):
            epsilon = 1/(t**(0.6)) # recommanded step 1/(t**(0.6))
            for i in range(n):
                S = S+2*x[i]*(y[i]-np.dot(x[i],omega/np.linalg.norm(omega,ord=2)))
            #omega = omega/np.linalg.norm(omega,ord=2) # normalization of omega at each step
            print(omega)
            omega = omega + epsilon*S/n # computation of next omega
    return omega 

# Initialisation of omega
omega_0 = np.array([np.random.random(),np.random.random()])
print("Initial values of omega")
print(omega_0)
grad_sto_risk(x_i,y_i,omega_0,50)

I tried to normalized the omega vector at each step but it doesn't work and it still diverge. Anyone has an idea to fix this ?
Thank you in advance for the help.

        