
# Probability of a sequence of words using BERT

I am using BERT (more specifically bert-large-cased) to get the probability of a token or multiple tokens in specific context.
If it's only one token, I just get the probability and if it's multiple tokens I get the product of their probabilities. This means that longer spans are in a sense penalised. Is there any known way to work around this issue?
For example in the following text, "Bodewin Claus Eduard Keitel" and "head of the Army Personnel Office" will result in very low probabilities because of their length.
text = "Bodewin Claus Eduard Keitel was a German general, head of the Army Personnel Office."

I've looked into BPC and perplexity but they don't seem to work for cases like this.

        