
# Way to control for child nodes in element which is given after xpath using lxml

I trid the following sample code in my pc
    from bs4 import BeautifulSoup
    from lxml import etree, html
    import requests
    URL = "https://en.wikipedia.org/wiki/Nike,_Inc."
    
    HEADERS = ({'User-Agent':
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \
                (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\
                'Accept-Language': 'en-US, en;q=0.5'})
    
    webpage = requests.get(URL, headers=HEADERS)
    soup = BeautifulSoup(webpage.content, "html.parser")
    dom = etree.HTML(str(soup))
    print(dom.xpath('//*[@id="firstHeading"]')[0].text)

But print out is empty
I checked about content dom text but it is None
On the other hand I checked about result as xpath using html.tostring()
So content is exsiting...
Actually, it is just sample so I would like to control the following codes,
    import pytest
    from bs4 import BeautifulSoup
    from lxml import etree, html
    import requests
    from urllib import request as rs
  

    def test_scraping():
        URL = "https://news.yahoo.co.jp/search?p=岸田文雄&ei=utf-8&categories=business"
    
        HEADERS = ({'User-Agent':
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \
                (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\
                'Accept-Language': 'en-US, en;q=0.5'})
    
        webpage = requests.get(URL, headers=HEADERS)
        soup = BeautifulSoup(webpage.content, "html.parser")
        dom = etree.HTML(str(soup))
        elements = dom.xpath("//li[@class='viewableWrap newsFeed_item newsFeed_item-normal newsFeed_item-ranking']")
        for element in elements:
            print(element.tag)
            if element.text is not None:
               .... <-- not working....


I can find a tag contet or another tags one using find function
But I would like to control it using another way
So let me know the way if you know it

        