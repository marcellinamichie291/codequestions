
# all_gather_object got stuck in pytorch DDP

Background: I'm trying train a model on separate GPU via pytorch DDP, and I want to gather local objects via function all_gather_object
Problem: my all_gather_object got stuck in the following code.
Code Version 1
import os
import logging
import torch
import torch.nn as nn
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader, TensorDataset

from util import *

logger = logging.getLogger("demo")
logger.addHandler(logging.FileHandler("demo.out"))
logger.setLevel(logging.INFO)

class ToyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(3, 3)
    def forward(self, x):
        return self.linear(x)

if __name__ == "__main__":
    '''torch setup'''
    reserve_gpu([2,3])
    dist.init_process_group("nccl")
    rank = dist.get_rank()
    device_id = rank % torch.cuda.device_count()

    '''train'''
    x = []
    for inst in range(12):
        x.append(torch.randn(3))
    x=torch.stack(x)
    dataset = TensorDataset(x)
    sampler = DistributedSampler(dataset=dataset)
    loader = DataLoader(
        dataset=dataset,
        num_workers=8,
        pin_memory=True,
        sampler=sampler
    )
    model = DDP(
        ToyModel().to(device_id),
        device_ids=[device_id],
    )
    metrics = {"rank":rank}
    # for epoch in range(1):
    #     for idx, x in enumerate(loader):
    #         logger.info(f'<{os.getpid()}>: {idx}, {x}')
    output = [None for _ in range(dist.get_world_size())]
    dist.all_gather_object(output, metrics)
    print(output)

Expected output:
[{rank:0},{rank:1}]
[{rank:0},{rank:1}]

But in fact, I got stuck in this function dist.all_gather_object(output, metrics)
Code Version 2
import os
import logging
import torch
import torch.nn as nn
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader, TensorDataset

from util import *

logger = logging.getLogger("demo")
logger.addHandler(logging.FileHandler("demo.out"))
logger.setLevel(logging.INFO)

class ToyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(3, 3)
    def forward(self, x):
        return self.linear(x)

if __name__ == "__main__":
    '''torch setup'''
    # reserve_gpu([2,3])
    dist.init_process_group("nccl")
    rank = dist.get_rank()
    device_id = rank % torch.cuda.device_count()
    gather_objects = ["foo", 12, {1: 2}] # any picklable object
    output = [None for _ in gather_objects]
    dist.all_gather_object(output, gather_objects[dist.get_rank()])
    print(output)

Traceback (most recent call last):
Traceback (most recent call last):
  File "demo.py", line 32, in <module>
  File "demo.py", line 32, in <module>
        dist.all_gather_object(output, gather_objects[dist.get_rank()])dist.all_gather_object(output, gather_objects[dist.get_rank()])

  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1649, in all_gather_object
  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1649, in all_gather_object
    all_gather(object_size_list, local_size, group=group)
  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2060, in all_gather
    all_gather(object_size_list, local_size, group=group)
  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2060, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1169, invalid usage, NCCL version 21.0.3
ncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams in a group, etc).
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1169, invalid usage, NCCL version 21.0.3
ncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams in a group, etc).
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3101) of binary: /data_HDD/zhuxingyu/anaconda3/envs/p11/bin/python
Traceback (most recent call last):
  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/lib/python3.8/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/lib/python3.8/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data_HDD/zhuxingyu/anaconda3/envs/p11/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
demo.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-11-02_12:45:22
  host      : ubuntu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3102)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-11-02_12:45:22
  host      : ubuntu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3101)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================


        