
# How to specify partitioning with the Spark-Snowflake connector? NumPartitions not supported?

Most JDBC connections I can specify partitioning directly using
sfOptions["numPartitions"]=10
sfOptions["partitionColumn"] = "***_ID"
sfOptions["lowerBound"] = lowerbound
sfOptions["upperBound"] = upperbound

But I can't find any of this in the snowflake documentation, only a reference to partition_size, which means you don't get to control how it's partitioned.
Is there really no way to control how data is partitioned into spark dataframes during the data load beyond this?

        