
# How to properly use OpenCV Mat in CUDA kernel

I want to apply my own filter to OpenCV Mat image in C++ in a CUDA kernel. I've seen I have to use GpuMat, but I don't understand how to refactor my code according to that library.
This is my host function:
void myBlur(Mat face, int w, int h, int THREADS){
    Mat table = summed_table(face, w, h);
    int blur_w = w-r-1, blur_h = h-r-1;

    /*CUDA WORK*/
    Mat *d_table, *d_face;
    int size = table.elemSize()*face.total(); //CV_32SC3
    
    cudaMalloc((void **)&d_table, size);
    cudaMalloc((void **)&d_face, size);

    cudaMemcpy(d_table, &table, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_face, &face, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(THREADS, THREADS);
    dim3 numBlocks((blur_w) / threadsPerBlock.x, (blur_h) / threadsPerBlock.y);

    myBlurKernel<<<numBlocks, threadsPerBlock>>>(d_face, d_table, blur_w, blur_h, r);
    
    cudaMemcpy(&face, d_face, size, cudaMemcpyDeviceToHost);

    cudaFree(d_table);
    cudaFree(d_face);
}

And this is my kernel:
__global__ void myBlurKernel(Mat *d_face, Mat *d_table, int width, int height, int r){
    int row = blockIdx.x*blockDim.x+threadIdx.x;
    int col = blockIdx.y*blockDim.y+threadIdx.y;
    int area = pow(2*r+1, 2);

    if (row < height && col < width){
        //ERROR
        d_face.at<Vec3i>(row,col) = d_table.at<Vec3i>(row+r, col+r) - d_table.at<Vec3i>(row+r, col-r-1) - d_table.at<Vec3i>(row-r-1, col+r) + d_table.at<Vec3i>(row-r-1, col-r-1);
        d_face.at<Vec3i>(row,col) /= area;
    }

}


        