
# TFRecordWriter with GZIP compression different file size each run writing same objects

I have a collection and I am trying to divide this collection into 3 different collections and write it to disk using TFRecordWriter and GZIP compression.  Using multiprocessing library within Python I am working on item at a time to assign it to one of the 3 new collections.
When I do this outside of Tensorflow and TFRecordWriter, the 3 collections are the same at the end (although perhaps items in each are ordered differently since it is multiprocessing).  So the idea works.  However, when I start to use TFRecordWriter with this compression, the filesizes are different each time I run the program.
Is this a property of gzip, or is there something about TFRecordWriter, or am I missing something else in my general approach?
Code with basic idea of what I am trying to do:
from multiprocessing import Lock, Manager, Process
import tensorflow as tf
from zlib import crc32

lock = Lock()

def bytes_to_float(b):
    # fastest of a number of options, but may not be random enough.   
    # from hashlib import sha256 could give you more random, while sacrificing speed.
    return float(crc32(b) & 0xffffffff) / 2**32

def single_process(filename, writers):
    r = bytes_to_float(filename.encode("utf-8"))

    #logging.debug('Random value is ' + str(r))
    if r <= 0.7:
        writer = writers["train"]
        #logging.debug('TRAIN')
    elif r <= 0.9:
        writer = writers["test"]
        #logging.debug('TEST')
    else:
        writer = writers["validate"]
        #logging.debug('VALIDATE')
    with lock:
        writer.append(filename)
    return writer

if __name__ == '__main__': 
    
    with open('multilist.txt', 'r') as fs:
        names = [l.strip() for l in fs.readlines()]
    
    proc_count = 0
    procs = []
    
    manager = Manager()
    write_holders = {"train": manager.list([]), "test": manager.list([]), "validate": manager.list([])}
    write_holders_shared = manager.dict(write_holders)
    
    for filename in names:
        proc_count += 1
        print('*****************************' + str(proc_count) + '***********************************')
        proc = Process(target=single_process, args=(filename, write_holders_shared))
        procs.append(proc)
        proc.start()
    
    for proc in procs:
        proc.join(timeout=0)
    
    while True:
        all_done = True
        for proc in procs:
            if proc.is_alive():
                all_done = False
                break
        if all_done == True:
            break
        
    with tf.io.TFRecordWriter("train.tfrecord.gz",
                              tf.io.TFRecordOptions(compression_type="GZIP")) as train_writer:
        for write_item in write_holders['train']:
            train_writer.write(write_item)
    with tf.io.TFRecordWriter("test.tfrecord.gz",
                                 tf.io.TFRecordOptions(compression_type="GZIP")) as test_writer:
        for write_item in write_holders['test']:
            test_writer.write(write_item)
    with tf.io.TFRecordWriter("validate.tfrecord.gz",
                                 tf.io.TFRecordOptions(compression_type="GZIP")) as validate_writer:
        for write_item in write_holders['validate']:
            validate_writer.write(write_item)


        