
# How to use integrated GPU while training XGBoost model?

Firstly, I want to say that I am new in this field and don't know much.
I have following laptop: "dell vostro 15 5510", with GPU: "Intel(R) iris(R) Xe Graphics"
I have installed xgboost with following code
pip install xgboost
now am trying to train a model on GPU:
param = {'objective': 'multi:softmax', 'num_class':22}
param['tree_method'] = 'gpu_hist'
bst = xgb.train(param, dtrain, 50, verbose_eval=True, evals=eval_set)

but it throws following error:
XGBoostError: [11:16:53] C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0ac76685cf763591d-1/xgboost/xgboost-ci-windows/src/gbm/gbtree.cc:611: Check failed: common::AllVisibleGPUs() >= 1 (0 vs. 1) : No visible GPU is found for XGBoost.

I have tried to execute same code on google colab and it worked perfectly well. That's why I am thinking maybe my laptop needs to have dedicated GPU instead of integrated. and I don't think it is a problem of installation because https://xgboost.readthedocs.io/en/stable/install.html#python claims that pip install xgboost have GPU support on Windows

        