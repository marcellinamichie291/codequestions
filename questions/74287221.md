
# Looping through a pandas dataframe - how to make code run faster?

I have a dataframe, df, with 43244 rows, and a txt file, text with 1107957 lines. The purpose of the following code is to evaluate entries in df, and return a word_id value if they are present in the text.
with open('text.txt') as f:
    text = f.readlines()

for index, row in df.iterrows():
    lemma_id = 0
    for lines in range(len(text)):
        word_row = text[lines].split()
        if word_row[2] == row['Word']:
            word_id = word_row[1]
    row['ID'] = word_id

However, this code would take an estimated 120 days to complete in my jupyter notebook, and I (obviously) want it to execute a bit more efficiently.
How do I approach this? Should I convert text into a dataframe/database, or is there another more efficient approach?

        