
# Training XOR using ADAM but loss is stuck at certain value

Here is my code I am a newbie to coding so I don't know whether there are certain mistakes in my algorithm or coding but anyways here is my code: for info I am using swish activation function and Square Loss function any help will be greatly appreciated please why am I not using numpy ? I had certain problems with memory sharing of arrays so I abandoned it.
import math as mh
import random as rnd
import matplotlib.pyplot as plt
import pandas as pd
Data = [
         [2,[[rnd.random(),rnd.random()],[rnd.random(),rnd.random()]],[[0,0],[0,0]],[[0,0],[0,0]],[rnd.random(),rnd.random()],[0,0],[0,0],[[0,0],[1,0],[0,1],[1,1]],["N/A"]]
        ,[1,[[rnd.random(),rnd.random()]],[[0,0]],[[0,0]],[rnd.random()],[0],[0],[],[]]
        ,["Output of Final layer",["→"],["→"],["→"],["→"],["→"],["→"],[],[]]
        ,["Expected Output of Final layer",["→"],["→"],["→"],["→"],["→"],["→"],[[0],[1],[1],[0]],["←"]]
       ]
η = 0.01
β1 = 0.9
β2 = 0.9
ε = 10**(-8)
φ = 1000
def Swish_MultiLayered_Perceptron(Data,η,β1,β2,ε,φ):
    count = 1
    def float_multiplication(I,X):
        lst = []
        for element in X:
            lst.append(element*I)
        return(lst)
    def dot_product(A,B):
        dot = 0
        for index in range(0,len(A)):
            dot = dot + (A[index]*B[index])
        return(dot)
    def Swish_Perceptron(X,W,b):
        return((dot_product(X,W)+b)*(1/(1+(mh.e**(-dot_product(X,W)-b)))))
    def Swish_Perceptron_Inverse(X,W,b):
        return(dot_product(X,W)+b)
    def multiply(A,B):
        lst = []
        for index in range(0,len(A)):
            lst.append(A[index]*B[index])
        return(lst)
    def divide(A,B):
        lst = []
        for index in range(0,len(A)):
            lst.append(A[index]/B[index])
        return(lst)
    def subtract(A,B):
        lst = []
        for index in range(0,len(A)):
            lst.append(A[index]-B[index])
        return(lst)
    def add(A,B):
        lst = []
        for index in range(0,len(A)):
            lst.append(A[index]+B[index])
        return(lst)
    def ones(A):
        lst = []
        while len(lst) != A:
            lst.append(1)
        return(lst)
    while count <= φ:
        stepL = 0
        stepI = 0
        stepN = 0
        while stepL < len(Data)-2:
            while stepI < len((Data[stepL])[7]):
                store1 = []
                store2 = []
                while stepN < (Data[stepL])[0]:
                    store1.append(Swish_Perceptron(((Data[stepL])[7])[stepI],((Data[stepL])[1])[stepN],((Data[stepL])[4])[stepN]))
                    store2.append(Swish_Perceptron_Inverse(((Data[stepL])[7])[stepI],((Data[stepL])[1])[stepN],((Data[stepL])[4])[stepN]))
                    stepN = stepN + 1
                stepN = 0
                stepI = stepI + 1
                (Data[stepL+1])[7].append(store1)
                (Data[stepL+1])[8].append(store2)
            stepI = 0
            stepL = stepL + 1
        stepL = -3
        stepW = 0
        stepC = 0
        Data_Copy = Data
        gradstore = []
        while stepL >= -(len(Data)):
            gradientstore = []
            while stepW < len((Data[stepL])[1]):
                if stepL == -3:
                    coeff1W = []
                    for index in range(0,len((Data[stepL+1])[7])):
                        coeff1W.append((((Data[stepL+1])[7])[index])[stepW]-(((Data[stepL+2])[7])[index])[stepW])
                else:
                    wght = []
                    for index in range(0,len((Data[stepL+1])[1])):
                        wght.append((((Data[stepL+1])[1])[index])[stepW])
                    if len(wght) > len(gradstore):
                        coeff1W = dot_product(gradstore,wght[0:len(gradstore)])
                    if len(wght) < len(gradstore):
                        coeff1W = dot_product(gradstore[0:len(wght)],wght)
                    if len(wght) == len(gradstore):
                        coeff1W = dot_product(gradstore,wght)
                Direct = []
                Inverse = []
                for index in range(0,len((Data[stepL+1])[7])):
                    Direct.append((((Data[stepL+1])[7])[index])[stepW])
                    Inverse.append((((Data[stepL+1])[8])[index])[stepW])
                coeff2W = add(divide(multiply(Direct,subtract(Inverse,Direct)),Direct),divide(Direct,Inverse))
                while stepC < len(((Data[stepL])[1])[stepW]):
                    coeff3W = []
                    for index in range(0,len((Data[stepL])[7])):
                        coeff3W.append((((Data[stepL])[7])[index])[stepC])
                    if stepL == -3:
                        (((Data_Copy[stepL])[2])[stepW])[stepC] = β1*(((Data_Copy[stepL])[2])[stepW])[stepC] + ((1-β1)*sum(multiply(coeff1W,multiply(coeff2W,coeff3W))))
                        (((Data_Copy[stepL])[3])[stepW])[stepC] = β2*(((Data_Copy[stepL])[3])[stepW])[stepC] + ((1-β2)*(sum(multiply(coeff1W,multiply(coeff2W,coeff3W))))**2)
                        mc = (1/(1-β1))*(((Data_Copy[stepL])[2])[stepW])[stepC]
                        vc = (1/(1-β2))*(((Data_Copy[stepL])[3])[stepW])[stepC]
                        (((Data_Copy[stepL])[1])[stepW])[stepC] = (((Data_Copy[stepL])[1])[stepW])[stepC] - ((η*mc)/(mh.sqrt(vc)+ε))
                    else:
                        (((Data_Copy[stepL])[2])[stepW])[stepC] = β1*(((Data_Copy[stepL])[2])[stepW])[stepC] + ((1-β1)*sum(float_multiplication(coeff1W,multiply(coeff2W,coeff3W))))
                        (((Data_Copy[stepL])[3])[stepW])[stepC] = β2*(((Data_Copy[stepL])[3])[stepW])[stepC] + ((1-β2)*(sum(float_multiplication(coeff1W,multiply(coeff2W,coeff3W))))**2)
                        mc = (1/(1-β1))*(((Data_Copy[stepL])[2])[stepW])[stepC]
                        vc = (1/(1-β2))*(((Data_Copy[stepL])[3])[stepW])[stepC]
                        (((Data_Copy[stepL])[1])[stepW])[stepC] = (((Data_Copy[stepL])[1])[stepW])[stepC] - ((η*mc)/(mh.sqrt(vc)+ε))
                    stepC = stepC + 1                                                      
                stepC = 0
                stepW = stepW + 1
                if stepL == -3:
                    gradientstore.append(sum(multiply(coeff1W,coeff2W)))
                else:
                    gradientstore.append(sum(float_multiplication(coeff1W,coeff2W)))
            gradstore = gradientstore
            for index in range(0,len((Data_Copy[stepL])[4])):                                                                                      
                ((Data_Copy[stepL])[5])[index] = β1*((Data_Copy[stepL])[5])[index] + ((1-β1)*sum(gradstore))
                ((Data_Copy[stepL])[6])[index] = β2*((Data_Copy[stepL])[6])[index] + ((1-β2)*(sum(gradstore)**2))
                mcb = (1/(1-β1))*((Data_Copy[stepL])[5])[index]
                vcb = (1/(1-β2))*((Data_Copy[stepL])[6])[index]
                ((Data_Copy[stepL])[4])[index] = ((Data_Copy[stepL])[4])[index] - ((η*mcb)/(mh.sqrt(vcb)+ε))
            stepW = 0
            stepL = stepL - 1
        Loss = 0
        for index2 in range(0,len(((Data[-2])[7])[0])):
            subloss = 0                                                           
            for index in range(0,len((Data[-2])[7])):
                subloss = subloss + 0.5*(((((Data[-2])[7])[index])[index2] - (((Data[-1])[7])[index])[index2])**2)
            Loss = Loss + subloss
        plt.plot(count,Loss,".",color="red")
        Data = Data_Copy
        n = 1
        if count != φ:
            while n < len(Data)-1:
                (Data[n])[7] = []
                (Data[n])[8] = []  
                n = n + 1
        count = count + 1
    plt.draw()         
    plt.show()
    return(pd.DataFrame(Data,columns=["No of Neurons in corresponding Layer","Weights of Each Neuron","Moving average of Gradient of Weights","Moving average of Square of Gradient of Weights","Bias of each Neuron","Moving average of Gradient of Bias","Moving average of Square Gradient of Bias","Input of current Layer/Output of Previous Layer","Inverse values of Input of current Layer/Output of Previous Layer"]))
Swish_MultiLayered_Perceptron(Data,η,β1,β2,ε,φ)

I tried various learning rate but all did it do was slow down to converging to same value I didn't mess so much with B1 and B2 but I think this is a good enough value.

        