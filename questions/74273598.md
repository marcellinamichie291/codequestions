
# Python spaCy custom tokenizer suffix and infix problem

I'm working on adding rules to the 'en_core_web_sm' spaCy nlp model's tokeniser, to correctly tokenise the string below:
s = "This string M=20kg/H=200mm"
into tokens ['This', 'string', 'M=', '20', 'kg', '/', 'H=', '200', 'mm']
But having '/' as an infix, and 'kg' as a suffix right before the infix complicates the situation. How can I make the right tokenisation rules?
This is my code:
import spacy
nlp = spacy.load('en_core_web_sm')
s = "This string M=20kg/H=200mm"
# Update suffix search 
from spacy.lang.char_classes import UNITS
unit_suffix_regex = r"(?<=[\d])({u})".format(u=UNITS)
suffixes = nlp.Defaults.suffixes
suffixes.append(unit_suffix_regex)
suffix_regex = spacy.util.compile_suffix_regex(suffixes)
nlp.tokenizer.suffix_search = suffix_regex.search

But it still gives the wrong results, and tokenises the string above into:
('TOKEN', 'This')
('TOKEN', 'string')
('TOKEN', 'M=20kg')
('INFIX', '/')
('TOKEN', 'H=200')
('SUFFIX', 'mm')

If I modify s into s = "This string M=20kg /H=200mm", then I get this output:
('TOKEN', 'This')
('TOKEN', 'string')
('TOKEN', 'M=20')
('SUFFIX', 'kg')
('TOKEN', '/H=200')
('SUFFIX', 'mm')

And 'kg' gets recognised as a suffix (I think this also happens without me adding the rule)
What is the problem here? How can I make this work?

        