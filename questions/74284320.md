
# python: Looping with itertools.conbinations on a lot of data is very time consuming

pandas: How do I remove duplicate rows by column similarity?
In the above question, how do I remove duplicate rows depending on the similarity of the values in the dataframe columns?
and I have completed the process I am looking for, but this code is very time consuming when there is a lot of data and there is no end in sight.
from itertools import combinations

def check_similar(s1, s2):
    return sum(1 for a, b in zip(s1, s2) if a != b)

matches = [
    s
    for s in combinations(df.thumbnail_hash.unique(), 2)
    if check_similar(*s) <= 5
]

# I thought the check_similar function was the cause, 
# but the following also takes a long time.
matches = [s for s in combinations(df.thumbnail_hash.unique(), 2)]

I have no idea how to improve the speed of this. How can I improve the speed?

        