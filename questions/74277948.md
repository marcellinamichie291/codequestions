
# XGBoost feature importance giving the results for 10 features

I am trying to use XGBoost as a feature importance tool. However, out of 84 features, I got only results for only 10 of them and the rest of them prints zeros. Do you know how to fix it?
This is my code and the results:
import numpy as np
from xgboost import XGBClassifier
from xgboost import plot_importance
from matplotlib import pyplot

X = data.iloc[:,:-1]
y = data['clusters_pred']

model = XGBClassifier()
model.fit(X, y)

sorted_idx = np.argsort(model.feature_importances_)[::-1]
for index in sorted_idx:
    print([X.columns[index], model.feature_importances_[index]])

['XYZ', 0.97976303]
['ABC', 0.008309732]
['ZZZ', 0.007930854]
['CGNK', 0.0011405549]
['TTT', 0.0011277349]
['PLT', 0.0007475067]
['HB', 0.00056899816]
['PBB', 0.00020151233]
['AGE', 0.000108826855]
['SEX', 0.0001012349]
['BLA', 0.0]
['STAT', 0.0]
['tRU', 0.0]
...


        