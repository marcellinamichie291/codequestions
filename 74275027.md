
# Is Pyspark good for an ETL Job?

Hi everyone I wonder something about Spark. We are using ODI (oracle data integrator) for our ETL jobs in my company. There is a case regarding completion speed of an etl job. This job basically does some joins, aggregations etc. between two layers fields ( it kind of from layer A to layer B ). This job completion speed is approximately 1.3 hour. I am learning Spark right now and I thought maybe we can do this job with spark and maybe it can be faster.. So, is that possible or not? Thanks in advance.
The speed of an etl job is very slow and I wonder, can I update it with pyspark and can that be faster with spark?

        